\documentclass{article}
\usepackage{pythonhighlight}
\usepackage{graphicx}

% TITLE PAGE CONTENT %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\labno}{02}
\newcommand{\labtitle}{EE208 Crawler}
\newcommand{\authorname}{Zhou Litao}
\newcommand{\studentno}{518030910407}
\newcommand{\classno}{F1803016}
% END TITLE PAGE CONTENT %%%%%%%%%%%%%%%%%%%%


\begin{document}

\begin{center}
{\LARGE \textsc{Laboratory No. \labno:} \\ \vspace{4pt}}
{\Large \textsc{\labtitle} \\ \vspace{4pt}} 
\rule[13pt]{\textwidth}{1pt} \\ \vspace{15pt}
{\large By: \authorname \\ \vspace{10pt}
No. \studentno \\ \vspace{10pt}
SJTU \classno \\ \vspace{10pt}
\today \vspace{20pt}}
\end{center}



\section{Introduction}

\subsection{Equipment}
\begin{itemize}
\item\textbf{Environment} Ubuntu 16.04 (on Virtual Machine)
\item\textbf{Language} Python 2.7.16 with packages as follows
	\begin{itemize}
	\item urllib 1.24.2
	\item beautifulsoup4 4.8.0
	\end{itemize}
\item\textbf{Tools} PyCharm 2019.2, Virtual Box
\end{itemize}
The Python script is also tested in Windows 10.

\subsection{Purpose}
In this experiment, there are three exercises for the construction of a crawler. First, we need to sign in the SJTU BBS and modify our self-introduction by means of POST requests. Second, we need to write a BFS function and return the graph structure as a crawling method. Finally, we should complete a simple crawler.

\subsection{Principle}

For the first exercise, the interactions we do with the website are in the form of requests (like GET, POST, etc.) in principle. So in order to manipulate our information board in SJTU BBS, all we need to do is to simulate a POST request in Python. This work can be easily implemented with the urllib module.

For the second exercise, a \textit{linear list} is used to store the vertices to be visited in the future. The linear list can be a stack, following the first-in-last-out rule, then the searching method will be DFS, since the closest vertex to the current vertex (which is also the latest added vertex) will be visited in the next step at once. The linear list can also be a queue, following the first-in-first-out rule, then the searching method will be BFS, since after the deeper vertices are added to the queue, they won't be visited until all the vertices above their levels have been visited. So with little modification to the given DFS function, we can write out a BFS function as the exercise requires.

For the third exercise, by combining the BFS or DFS program as we've written in exercise 2, all we have to do is to give concrete implementation of the get\_page() function and get\_all\_links() function. With urllib and BeautifulSoup parser, we can write out theses functions to complete a simple crawler.


\section{Procedure}
\subsection{Exercise 1}

\subsubsection{Solution}
In this exercise, our goal is to return all the URLs (in the form of <a href...>) from a given page. With the HTML tree built by BeautifulSoup, all we have to do is to fetch all the URLs from the tree. First, we should locate to the <a> label and get the URL from its `href' property.

\paragraph{URL Matching} According to HTML standards, the `href' property can be a URL (with or without fragment identifiers), or a JavaScript segment. In order to distinguish URL from others, a Regular Expression is used here to match URLs. A remarkable characteristic in standard URL is that it begins with a protocol name which ends with `://'. So the Regular Expression can simply just check whether the property includes `://'   so as to exclude JavaScript segments out. Since the contents to be sorted is limited, this easy Regular Expression is considered to be enough for our sorting work.

\paragraph{Fragment Identifier Processing} Fragment identifiers are allowed in URL expressions, which can be introduced by a hash mark at the end of a URL. It is typically used to identify a portion of that document. For web crawlers, however, this part of the URL is unnecessary, since whether a URL has a fragment identifier or not, it indicates the same page. Therefore, for every URL we have collected, a string-processing step is designed to remove the identifier part. When the URL is added to the set, overlapping ones will be ignored. Another thing to note is that string.strip() is used to remove the whitespace at the head or end of the URL so that the URLs are uniformed.

\paragraph{About Protocol-relative URLs} Protocal-relative URLs begins with `//', indicating that the page it links uses the same protocal as the current page. They point to real pages that store information, and should be collected. However, due to the limitation of our function interface, there is no way to know the original URL of the current HTML document in the function body. Hence, non-standard URLs are aborted in this exercise.

The solutions above are implemented in the Python script as follows.
\begin{python}
def parseURL(content):
    urlset = set()
    soup = BeautifulSoup(content,features='html.parser')
    for i in soup.findAll('a',{'href': re.compile('://')}):
        j = i['href'].split('#')[0].strip()
        urlset.add(j)
\end{python}

\subsubsection{Results}
The test results can be found at graph \ref{img:1.1} and \ref{img:1.2}.

\begin{figure}[htbp]
\centering
\includegraphics[width=9.5cm]{img/test1_1.png}
\caption{parseURL in PyCharm}
\label{img:1.1}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=9.5cm]{img/test1_2.png}
\caption{parseURL in console}
\label{img:1.2}
\end{figure}

\subsection{Exercise 2}

\subsubsection{Solution}

In this exercise, we should extract all the image addresses from the HTML tree. According to the requirement, we only need to consider the images in forms of <img src=``..">, so we first ask BeautifulSoup to fetch all the `src' properties in img labels. Also, for each address, we use string.split() for sake of uniformity. The codes of parseIMG are similar to parseURL, except that the findAll() condition is changed.

\begin{python}
    for i in soup.findAll('img'):
        imgset.add(i['src'].strip())
    return imgset
\end{python}

\paragraph{About Relative Paths} Similar to what we've discussed previously in Exercise 1, due to the lack of the original URL, it's hard to process relative paths within the parseIMG function. However, we can write a new function to process the relative paths. The codes of this function are listed as follows.

\begin{python}
def URL_set_uniform(URLset,url):
    urlseg = urlparse.urlparse(url)
    uniURLs = set()
    for j in URLset:
        if re.match('^//',j):         #case 1: relative-protocal URL
            uniURLs.add(urlseg.scheme+ ':'+j)
        elif (not re.match('://',j)): #case 2: relative path
            uniURLs.add(urlparse.urljoin(url,j))
        else:                         #case 3: standard URL
            uniURLs.add(j)
    return uniURLs
def main():
    ....
    urls = parseIMG(content)
    write_outputs(URL_set_uniform(urls,url), 'res2.txt')
\end{python}

Here, we import the urlparse module in Python in order to get the current URL scheme and deal with relative paths. First, urlparse() can split the URL into several meaningful parts, which includes the protocal of the current page. Then for every URL in the URLset, if the URL begins with `//' (i.e. relative-protocal URL), then we add the protocal before the string. If this is not the case and the URL can't match standard URL, then it should be a relative path. the urljoin() function can help us with the joining work. A new set is constructed in case of overlapping addresses after this operation. 

\subsubsection{Results}

The test results can be found at graph \ref{img:2.1} and \ref{img:2.2}.

\begin{figure}[htbp]
\centering
\includegraphics[width=9.5cm]{img/test2_1.png}
\caption{parseIMG in PyCharm}
\label{img:2.1}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=9.5cm]{img/test2_2.png}
\caption{parseIMG in console}
\label{img:2.2}
\end{figure}



\subsection{Exercise 3}

\subsubsection{Solution}

By analyzing the information we want to extract, we can find that the contents to collect is located in a <div> named `box'. BeautifulSoup can easily locate to this <div> in the HTML tree. The findAll() command is written as follows

\begin{python}
def parseZhihuPic(url,content):
    pics = set()
    soup = BeautifulSoup(content,features='html.parser')
    divs = soup.findAll("div", "box")
    ...
\end{python}

Then by visiting the contents of this <div> node, we can collect the image addresses, text labels and URL link in every column. Note that the URL link here in Zhihu Daily is in the form of relative path. So when we are collecting the URL link, we change it into its full path. The 3 elements are packed in a tuple, and added into the result set.
\begin{python}
    for i in divs:
        pics.add((i.img['src'],i.span.string,url+i.a['href']))
    return pics
\end{python} 

In order to meet the requirement of the output format, we should rewrite the write\_outputs() function.
\begin{python}
def write_outputs(pics, filename):
    with open(filename, 'w') as f:
        for pic in pics:
            for j in range(3):
                f.write(pic[j])
                f.write('\n')
\end{python}

\subsubsection{Results}

In Exercise 3, no parameter in console is required. So whether in IDE or in console, the script will have the same effect. The test results can be found at graph \ref{img:3.1} and \ref{img:3.2}.

\begin{figure}[htbp]
\centering
\includegraphics[width=9.5cm]{img/test3_1.png}
\caption{parseZhihuPic in PyCharm}
\label{img:3.1}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=9.5cm]{img/test3_2.png}
\caption{parseZhihuPic in console}
\label{img:3.2}
\end{figure}

\section{Discussion \& Conclusion}
\paragraph{Overview}
Through the practice of this laboratory, I have gained a general knowledge of how the HTML parser works and how we can make use of BeautifulSoup, a parser module in Python, to help us collect specific information from a webpage. I've also learnt some web data processing skills in the course of this lab.

\paragraph{Thoughts}
The structure and contents of different webpages vary a lot. When we are writing a parser project, it is important that we should bear in mind to "Make a concrete analysis of concrete conditions".  Only if we figure out how the contents are structured in the page can we locate our objective information correctly and efficiently. For example, in parseIMG, we should provide 3 cases based on how the image address looks like in order to return a satisfactory result.

\paragraph{Innovations}
In this lab, I have written a new useful function called URL\_ set\_uniform (See Exercise 2). Given a set of addresses (in the form of relative-protocal URLS, relative paths or standard URLs), it can return a set of addresses in their standard URL form. With this function, our functions can be more portable. I believe this will help a lot when we are dealing with further work such as crawlers.


\paragraph{Problems \& Solution}
The most encountered errors in this lab is about Coding. An efficient way to resolve this issue is to reload the sys module at the beginning of every Python script, as the following codes show.

\begin{python}
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
\end{python}

\end{document}

