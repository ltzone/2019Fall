\documentclass{article}
\usepackage{pythonhighlight}
\usepackage{graphicx}
\usepackage{ctex}
\usepackage[left=3cm,top=3cm,right=3cm]{geometry}
\usepackage{hyperref}
% TITLE PAGE CONTENT %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\labno}{05}
\newcommand{\labtitle}{EE208 Lucene}
\newcommand{\authorname}{周李韬}
\newcommand{\studentno}{518030910407}
\newcommand{\classno}{F1803016}
% END TITLE PAGE CONTENT %%%%%%%%%%%%%%%%%%%%


\begin{document}

\begin{center}
{\LARGE \textsc{Laboratory No. \labno:} \\ \vspace{4pt}}
{\Large \textsc{\labtitle} \\ \vspace{4pt}} 
\rule[13pt]{\textwidth}{1pt} \\ \vspace{15pt}
{\large By: \authorname \\ \vspace{10pt}
No. \studentno \\ \vspace{10pt}
SJTU \classno \\ \vspace{10pt}
\today \vspace{20pt}}
\end{center}



\section{实验准备}

\subsection{实验环境}
\begin{itemize}
\item\textbf{Environment} Ubuntu 16.04 (on Virtual Machine)
\item\textbf{Language} Python 2.7.16 with packages as follows
	\begin{itemize}
	\item urllib 1.24.2
	\item beautifulsoup4 4.8.0
	\item lucene 4.9.0
	\end{itemize}
\item\textbf{Tools} PyCharm 2019.2, Virtual Box
\end{itemize}

\subsection{实验目的}
本实验分为两部分，在第一部分，我们需要在原有搜索引擎的基础上加以改进，实现一个能够支持“site:”域名条件检索的搜索引擎。在第二部分，我们需要实现一个基于图片周围文本进行图片检索的搜索引擎。

\subsection{实验原理}
\label{sec:principle}

在第一部分的实验中，对域名检索而言，我们在Lab5中搭建的搜索引擎是不够的。这是因为该搜索引擎的索引中，并不存在我们需要检索的对象site，因此，我们需要更新原有的索引。在这里我们利用Lucene中的Term包匹配原来索引库中的文档，随后采用indexWriter中的deleteDocument()函数进行删除。再重新添加包含site域的索引文档。对检索程序部分，我们需要对输入的检索命令进行处理，提取其中分别对应site和content域的内容，并进行词法、语法分析，进而构造符合用户输入需求的query对象进行查询，才能实现满足实验目标的搜索引擎。

在第二部分的实验中，我们爬取了“豆瓣电影”（域名：movie.douban.com)站点下一定数量的电影、人物主页，针对页面的特征提取了这类网页上的主要电影、人物图片，将图片地址、网页URL、周边文字信息等信息有序地输出保存。随后，我们采用与此前实验类似的方法进行索引、搜索程序的构造，达到图片检索的目的。实验的具体实现细节将在下文展开。


\section{实验步骤}

\subsection{支持域名的搜索引擎}

\subsubsection{Solution}

解析包含“site:”条件的命令函数parsecommand()已在示例代码中给出并被本实验使用，对其他信息的处理、索引、搜索等过程已经在Lab4中实现，在本文中不再详述。

\paragraph{索引配置}
注意到网页的域名site内容是一个以“.”为间隔标记的若干英文字符串，因此中文分词器对site信息不会产生空格的分词效果，也就不会在analyzer阶段被WhiteSpaceAnalyzer识别。因此在本实验中，我们改用SimpleAnalyzer作为分词器，该分词器能够识别非字符的符号作为分词的依据，在这里既可以对中文空格进行分词，也可以对英文域名间的句点进行分词，满足本实验的要求。对于site的fieldType选择，由于它是用户的检索对象之一，因此也需要被索引、分词。此外，由于对域名的检索通常要求连续字段的匹配，因此在索引中记录词频和位置也是十分重要的，为了方便实验的检验，我们也将域名存储了下来，实际应用中是可以去除的。本实验中的fieldType配置如下表所示。
\begin{table}[h]
\begin{center}
\begin{tabular}{cccccc}
\hline
\textbf{Field Type} & \textbf{Field Name} & \textbf{Indexed} & \textbf{Stored} & \textbf{Tokenized} & \textbf{\begin{tabular}[c]{@{}c@{}}Record Freq\\ \& Position\end{tabular}} \\ \hline
\textbf{t1}         & path, url, name     & N                & Y               & N                  & N                                \\
\textbf{t2}         & contents            & Y                & N               & Y                  & Y                                \\
\textbf{t3}         & titles              & Y                & Y               & Y                  & Y                                \\
\textbf{t4}         & site                & Y                & Y               & Y                  & Y                                \\ 
 \hline
\end{tabular}
\end{center}
\end{table}

\paragraph{更新索引}
与Lab4中创建索引一样，我们通过遍历爬虫的结果目录index.txt对索引进行更新操作。更新索引的大部分代码与创建索引类似。不同之处首先在于我们要在创建新的索引前删除原有文档中的索引，我们利用Term工具匹配当前文件名对应的索引条目并删除。随后，我们为site域添加当前网页的域名，域名信息可以通过对当前URL调用urlparse库得到。更新索引的主要代码如下所示。

\begin{python}
def get_site(url):                                        # 从URL中提取域名
    return urlparse(url).netloc

def indexDocs(self, root, writer):
    while True:
        t = indextxt.readline()
        if (len(t) == 0):
            return
        ...
        URL = t[0]
        try:
            writer.deleteDocuments(Term("name", filename))# 匹配并删除当前文档的原索引
            ...
            doc.add(Field("site", get_site(URL), t4))     # 在索引文档中添加site域信息
            ...
        except Exception, e:
            print "Failed in indexDocs:", e
\end{python}

\paragraph{搜索程序}
实验材料中已为我们提供了一个处理用户输入的函数parsecommand()，它能将用户输入的命令转换成一个以检索field为key，以用户输入的关键词为value的dict数据结构。在此基础上，我们首先要对用户查询中文contents的关键词做分词处理，留待检索时被SimpleAnalyzer分词解析。随后对command\_dict进行遍历，将满足用户需求的field和keyword依次添加到最终用于检索的querys中，我们的添加结合方式为MUST，也就是说要同时满足所有条件的文档才会被输出。接下来用类似Lab4的方法，在此前构造的索引中对该querys进行检索，即可达到实验的目的。

\begin{python}
command_dict = parseCommand(command)                # 调用parsecommand函数，获得command_dict
seg_list = jieba.cut(command_dict['contents'])
command_dict['contents'] = (" ".join(seg_list))
querys = BooleanQuery()
for k,v in command_dict.iteritems():
    query = QueryParser(Version.LUCENE_CURRENT, k,
                        analyzer).parse(v)
    querys.add(query, BooleanClause.Occur.MUST)
\end{python}

\subsubsection{Results}

构造索引过程截图如图\ref{fig:siteindex}所示，搜索结果示例如\ref{fig:sitetest}所示，当限定域名为sh.sina.com.cn时，排在前列的搜索结果都是新浪上海域名下的网页。

\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/siteupdate.png}
\caption{构造索引过程}
\label{fig:siteindex}
\end{figure}


\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/sitesearch2.png}
\caption{搜索结果示例}
\label{fig:sitetest}
\end{figure}




\subsection{图片搜索引擎}

\subsubsection{Solution}
在本实验中，我们选取豆瓣电影网页（域名：movie.douban.com）为检索对象，爬取一定数目的电影和名人主页，获取其中的图片。

\paragraph{爬取豆瓣电影网页}
我们首先要针对性地爬取一定数目的豆瓣电影网站。此前我们的爬虫是在全网以广度优先遍历的方式进行爬取的，为了实现定向爬取，我们可以对Lab3中的并行化爬虫做一些改动。当我们在处理URL时，我们可以限制加入待爬队列的URL。我们的实现方式是利用urlparse库获取URL中的域名（netloc），只有当域名是豆瓣电影时加入队列。我们将这一步判断条件加在了URL标准化的函数中，这样可以保证每一个爬取到的网页都会被判断是否符合爬取条件。

另外需要注意的一点是我们最终输出的URL是通过urlparse的元素组合起来的，之所以不直接采用原本的URL原因是因为豆瓣在设计网页时，会加入一定的参数，如访问来源from、页面标签tag等，这会造成同一内容的网页被多次爬取和解析，因此我们通过连接urlparse结果各部分的方式，来移除不必要的参数。代码如下。

\begin{python}
def URL_set_uniform(URLset,page):
    urlseg = urlparse.urlparse(page)
    uniURLs = set()
    for i in URLset:
        j = i['href'].split('#')[0]      # 移除页面标签
        ......                           # 将相对路径转化为绝对路径
        jseg = urlparse.urlparse(j)      # 获得当前页面的域名，仅当域名是豆瓣电影时加入队列
        if (jseg.netloc == 'movie.douban.com'):
            uniURLs.add(jseg.scheme+'://'+jseg.netloc+jseg.path)
    return uniURLs
\end{python}


\paragraph{提取图片信息}
通常来说图片的信息集中在图片本身替代文字alt、以及图片周围的文本中，这些信息的提取可以通过取图片的父节点，调用get\_text函数来实现。但在本实验中，由于爬取时已经限制了豆瓣电影网页的范围，我们可以针对性地为一类具有相同结构的网页设计特定的提取图片及信息函数，这样可以达到更好的检索效果。这里以豆瓣电影中的电影主页(http://movie.douban .com/subject/...)为例，展示我们针对性提取信息的方法。提取subject页面信息的代码如下，该函数的第一个参数是经过beautifulsoup解析的HTML树，第二个参数是输出文档的文件。

\begin{python}
def extract_subject(soup, output):
    global COUNT
    title = soup.h1.get_text(strip=True).split('(')[0]          # 网页标题，去除（豆瓣）后缀
    titleseg = jieba.cut(title)
    title = ' '.join(titleseg)                                  # 对网页标题分词

    # 电影主题图
    output.write(soup.find(id = "mainpic").img['src']+'\n')     # 电影主题图地址打印在第一行
    output.write(title+' ')
    infoseg = jieba.cut(soup.find(id="info").get_text(strip=True))
    infos = " ".join(infoseg)              # title+infos作为电影相关信息的分词结果打印在第二行
    output.write(infos+'\n')
    COUNT += 1
    
    # 电影主要卡司
    for celebrity in soup.select(".celebrity"):
        imgsrc = celebrity.select('.avatar')[0]['style'].split('(')[1].split(')')[0]
        output.write(imgsrc+'\n')                               # 电影卡司人物图打印在第一行
        output.write(title+' ')
        textseg = jieba.cut(' '.join(celebrity.select(".info")[0].get_text().split('\n')))
        output.write(" ".join(textseg))                         # 电影卡司信息打印在第二行
        output.write('\n')
        COUNT += 1
        
    # 电影相关图
    related_session = soup.find(id = "related-pic")
    for pic in related_session.find_all('img'):
        imgsrc = pic['src']
        ...    # 在output中第一行写入imgsrc，第二行写入和标题图一样的title，infos等信息
        COUNT += 1

    output.write('EOF')
\end{python}

如图\ref{fig:pagestructure}所示，在subject页面中，我们主要提取三类图片——电影主宣传图、演员图和电影相关图。每个页面有一张电影主宣传图，位于一个id为mainpic的容器中，可以用soup.find方法提取出来。电影的演职员表、地区、时间、译名等位于图片右侧id为info的信息栏中，我们用get\_text函数和结巴中文分词器进行处理，类似地，我们也可以用soup.find方法提取id为related-pic的电影相关图，将电影信息作为描述该图片的文本。对演职员图，它们分布于一个class为celebrity的容器中，soup.find方法不支持对class的查找，为此我们该用soup.select方法，匹配class的名称。另外需要注意的一点是演员图片的图片地址是作为CSS参数的一部分包含在标签里的，因此我们需要对标签做一些字符串上的处理才能得到演员的照片。倘若使用一般化的方法，我们是难以从这里提取出演职员照片这一重要信息的。

\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/pagestructure.png}
\caption{subject页面的结构和图片信息示意}
\label{fig:pagestructure}
\end{figure}

\paragraph{建立图片文档}

首先，我们写了一个简短的获取页面类型的函数。我们注意到豆瓣电影的URL十分有规律，它是以“http://movie.douban.com/页面类型/页面编号”的方式呈现的，我们通过urlparse库提取出URL中的path，并取第一个“/”前的文字，就获得了页面的类型，进而利用该类型页面共有的函数来建立图片文档。
\begin{python}
# 一个用于获取豆瓣电影域名下页面类型（即path的第一级目录）的函数
def getPageType(URL):
    urlseg = urlparse.urlparse(URL)
    if len(urlseg.path)>0:
        return urlseg.path.split('/')[1]
    else:
        return ''
\end{python}

随后，我们遍历已爬取的文件目录index.txt，对符合提取图片函数要求的页面（在本实验中，有subject和celebrity两类网页），我们对解析后的HTML树调用对应的提取函数，并将提取结果和页面标题、URL一并输出在另一个目录下的同名文件中。在主程序中，我们定义了一个全局变量COUNT，用来统计已被提取的图片数目，伪代码如下所示。

\begin{python}
def PagesToDocs(root,index,storeDir):
    while (True):
        从index的一行中获取URL，filename和pagetype，如果该行为空则break
        try:
            path = os.path.join(root, filename)
            content = open(path)
            soup = BeautifulSoup(content, features='lxml')
            content.close()
            if pagetype in ["subject","celebrity"]:
                output = open(os.path.join(storeDir, filename), 'w')
                从content中提取标题title
                在output中写入title，URL
                调用globals()['extract_%s' % pagetype](soup, output)
                将当前文件名写入docindex文件中
        except Exception, e:
            print "Failed in indexDocs:", e

if __name__ == "__main__":
    COUNT = 0                                # COUNT是一个全局计数器，统计被存储的图片数
    PagesToDocs("html","index.txt","docs")
    print "total images collected", COUNT
\end{python}


\paragraph{图片索引与搜索}

在获得了图片信息后，我们就可以为此前创建的信息文档建立索引和搭建搜索程序。建立索引时的FieldType配置如下，对contents和titles的分词器选用了支持空格和特殊符号分割的SimpleAnalyzer。

\begin{table}[h]
\begin{center}
\begin{tabular}{cccccc}
\hline
\textbf{Field Type} & \textbf{Field Name} & \textbf{Indexed} & \textbf{Stored} & \textbf{Tokenized} & \textbf{\begin{tabular}[c]{@{}c@{}}Record Freq\\ \& Position\end{tabular}} \\ \hline
\textbf{t1}         & imgurl, url, name   & N                & Y               & N                  & N                                                                          \\
\textbf{t2}         & contents            & Y                & N               & Y                  & Y                                                                          \\
\textbf{t3}         & titles              & Y                & Y               & Y                  & Y                                                                          \\ \hline
\end{tabular}
\end{center}
\end{table}

在建立索引的过程中，通过按顺序地读取图片信息文档，我们就可以依次为索引文档不同的Field添加相应信息。索引与搜索程序的Python脚本与Lab4类似，本段不作详述。

\subsubsection{Results}

爬取网页的过程如图\ref{fig:crawl_images}所示。实验中我们以movie.douban.com为seed，爬取了约400个网页。

\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/crawl_images.png}
\caption{爬取豆瓣电影网页过程截图}
\label{fig:crawl_images}
\end{figure}

建立图片文档的脚本位于PagetoDocs.py文件中，建立过程和文档结果示例如图\ref{fig:pagetodocs}和\ref{fig:docexample}所示。建立索引的脚本位于UpdateFiles.py文件中，过程如图\ref{fig:indexpict}所示。

\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/getpict.png}
\caption{建立图片文档过程截图}
\label{fig:pagetodocs}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/pictdocs.png}
\caption{图片文档结果示例}
\label{fig:docexample}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/indexing.png}
\caption{为图片文档建立索引过程截图}
\label{fig:indexpict}
\end{figure}

搜索结果示例如图\ref{fig:search1pict}和\ref{fig:search2pict}所示。

\begin{figure}[htbp]
\centering
\includegraphics[width=8.5cm]{img/search.png}
\includegraphics[width=8.5cm]{img/search2.png}
\caption{图片搜索示例}
\label{fig:search1pict}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/search3.png}
\caption{图片搜索示例}
\label{fig:search2pict}
\end{figure}

\section{实验总结}
\paragraph{概述}
本实验中，我们实现了一个支持site搜索的搜索引擎，能够对搜索结果的域名加以限定。此外，我们还针对豆瓣电影搭建了一个图片搜索引擎，能够匹配图片周围的文本进行检索。

\paragraph{感想}
通过本次实验的学习，我更加熟练地掌握了搭建搜索引擎的技巧，并且通过模拟site搜索的实验学会了更新索引、处理复杂query的方法。在搭建图片搜索引擎的过程中，我对网页爬取、解析的能力也得到了进步。

\paragraph{创新}
在本实验中，不同于普遍方法，我针对豆瓣电影网站本身的特点，写了extract\_subject，extract\_celebrity两个解析特定类型页面获取图片的函数。能够有效避免图片与文字信息不匹配的问题，并且还能在一些特定场合（如图片url作为背景被嵌入css样式中）有效提取需要的图片，更加提高了索引文档的可靠性和准确性，能够提升搜索引擎的表现。此外，HtmltoDocs这一脚本也具有一定的扩展性，当我们需要爬取更多其他类型的页面时，我们只需要提供新的extract\_xxx函数，并在PagetoDocs函数中的页面类型参数列表中添加该类型名称即可。

\paragraph{问题}
本实验在处理site检索过程中，遇到的一个问题是域名无法做到部分匹配。如输入site:sina. com.cn时，只会输出域名是sina.com.cn的网址，其他类似www.sina.com.cn的网页不会被输出，如图\ref{fig:sitetest2}所示。分析原因，我们原本采用的分词器是WhiteSpaceAnalyzer，这一分词器能对我们经过空格分隔的中文分词结果有效，但并不会对网站域名做任何分词的操作，因此我们改用SimpleAnalyzer解决了这一问题。这一问题的出现和解决提示我们在构造索引的过程中，选择合适的分词方式或分词器十分重要，这极大关系到搜索引擎的效率、表现乃至正确率。

\begin{figure}[htbp]
\centering
\includegraphics[width=14.5cm]{img/sitesearch.png}
\caption{使用WhiteSpaceAnalyzer的检索结果}
\label{fig:sitetest2}
\end{figure}



\end{document}

